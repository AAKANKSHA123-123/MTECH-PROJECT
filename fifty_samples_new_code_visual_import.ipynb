{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fifty_samples_new_code_visual_import.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAKANKSHA123-123/MTECH-PROJECT/blob/main/fifty_samples_new_code_visual_import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA_xQ5F-HvaM",
        "outputId": "9e49ff28-cd4a-4262-9f33-d0b768e8e6f9"
      },
      "source": [
        "!pip install ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipynb\n",
            "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: ipynb\n",
            "Successfully installed ipynb-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrFXn89tN09w",
        "outputId": "dbce4a5f-f03e-4fd8-8973-7a32085e4feb"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGpYo_6w5rz1",
        "outputId": "0d7e008f-da2f-46ed-da4d-1a98e9f3471f"
      },
      "source": [
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting import-ipynb\n",
            "  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=334980ba40f2d2837e2c38464de6aee5a45bdcc14ea12e584b583981dda4e06d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/5e/dc/79780689896a056199b0b9f24471e3ee184fbd816df355d5f0\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rte4EaYr9aRh"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5KvECyNYc9o",
        "outputId": "eaa3c41a-b296-4735-f382-88acf2859dba"
      },
      "source": [
        "%cp /content/drive/MyDrive/newresultsdata.csv\n",
        "%cp /content/drive/MyDrive/Colab \\ Notebooks/Newcodevisualvgg16.ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: missing destination file operand after '/content/drive/MyDrive/newresultsdata.csv'\n",
            "Try 'cp --help' for more information.\n",
            "cp: cannot stat '/content/drive/MyDrive/Colab': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnDxHbBkdkyM",
        "outputId": "5243dd9b-5132-44e8-dd37-d0316a2c584d"
      },
      "source": [
        "!python /content/drive/MyDrive/Newcodevisualvgg16.ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/drive/MyDrive/Newcodevisualvgg16.ipynb': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leaQvCFPJuN2"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl4a6mMuJqbP"
      },
      "source": [
        "##fixing numpy RS\n",
        "np.random.seed(42)\n",
        "\n",
        "##fixing tensorflow RS\n",
        "tf.random.set_seed(32)\n",
        "\n",
        "##python RS\n",
        "rn.seed(12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "GCn5-EK-Kzq9",
        "outputId": "39c27d96-a2d6-43cb-a355-d27855dc9f30"
      },
      "source": [
        "'''import h5py\n",
        "filename = \"/content/drive/MyDrive/newresults/dataset_train.hdf5\"\n",
        "\n",
        "with h5py.File(filename, \"r\") as f:\n",
        "    # List all groups\n",
        "    print(\"Keys: %s\" % f.keys())\n",
        "    a_group_key = list(f.keys())[0]\n",
        "\n",
        "    # Get the data\n",
        "    data = list(f[a_group_key])'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import h5py\\nfilename = \"/content/drive/MyDrive/newresults/dataset_train.hdf5\"\\n\\nwith h5py.File(filename, \"r\") as f:\\n    # List all groups\\n    print(\"Keys: %s\" % f.keys())\\n    a_group_key = list(f.keys())[0]\\n\\n    # Get the data\\n    data = list(f[a_group_key])'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s7OR9KGVb25",
        "outputId": "b4a47fed-69db-4eb7-b634-7311115b52c1"
      },
      "source": [
        "!pip install tensorflow-io"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-io\n",
            "  Downloading tensorflow_io-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.7 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.21.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 21.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.7.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.37.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.12.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.17.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.41.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.6.0)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n",
            "Successfully installed tensorflow-io-0.21.0 tensorflow-io-gcs-filesystem-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFGJ7Jo9Vgh1"
      },
      "source": [
        "import tensorflow_io as tfio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6Q_hrbUxmWv",
        "outputId": "354a3cbd-40c1-4c97-8f97-107f566a4d5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7n6l_qZxQSk"
      },
      "source": [
        "import h5py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M-DmWXqw6VV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644450c6-6d80-4f23-9683-f0c8c38abf6d"
      },
      "source": [
        "with h5py.File('/content/drive/MyDrive/newresults/fiftytraining.hdf5', 'r') as hf:\n",
        "    ls = list(hf.keys())\n",
        "    print('list of dataset in this file: \\n',ls)\n",
        "    img_data_tr = hf.get('imgg_train')\n",
        "    img_data1_tr = np.array(img_data_tr)\n",
        "    print(img_data1_tr.shape)\n",
        "    ques_data_tr = hf.get('quess_train')\n",
        "    ques_data1_tr = np.array(ques_data_tr)\n",
        "    print(ques_data1_tr.shape)\n",
        "    anss_data_tr = hf.get('anss_train')\n",
        "    anss_data1_tr = np.array(anss_data_tr)\n",
        "    print(anss_data1_tr.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list of dataset in this file: \n",
            " ['anss_train', 'imgg_train', 'quess_train']\n",
            "(916,)\n",
            "(916, 21)\n",
            "(916, 324)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGq-JXVzVdxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312705c0-6069-496c-946c-cbf8a7bf394b"
      },
      "source": [
        "with h5py.File('/content/drive/MyDrive/newresults/fiftyvalidation.hdf5', 'r') as hf:\n",
        "    ls = list(hf.keys())\n",
        "    print('list of dataset in this file: \\n',ls)\n",
        "    img_data_val = hf.get('imgg_val')\n",
        "    img_data1_val = np.array(img_data_val)\n",
        "    print(img_data1_val.shape)\n",
        "    ques_data_val = hf.get('quess_val')\n",
        "    ques_data1_val = np.array(ques_data_val)\n",
        "    print(ques_data1_val.shape)\n",
        "    anss_data_val = hf.get('anss_val')\n",
        "    anss_data1_val = np.array(anss_data_val)\n",
        "    print(anss_data1_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list of dataset in this file: \n",
            " ['anss_val', 'imgg_val', 'quess_val']\n",
            "(230,)\n",
            "(230, 21)\n",
            "(230, 324)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_88hkW4_H6XX"
      },
      "source": [
        "datrain_tens = tf.data.Dataset.from_tensor_slices((img_data1_tr,ques_data1_tr,anss_data1_tr))\n",
        "daval_tens = tf.data.Dataset.from_tensor_slices((img_data1_val,ques_data1_val,anss_data1_val))\n",
        "#print(type(datrain_img))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFoFW-c7avDw"
      },
      "source": [
        "def createDataset2(image_paths,question_vector,answer_vector):\n",
        "    dataset_input = tf.data.Dataset.from_tensor_slices((image_paths, question_vector.astype(np.float32)))\n",
        "    dataset_output = tf.data.Dataset.from_tensor_slices((answer_vector.astype(np.float32)))\n",
        "    # using map to load the numpy files in parallel\n",
        "    dataset_input = dataset_input.map(lambda img, ques : tf.numpy_function(get_imageTensor, [img, ques], [tf.float32, tf.float32]),\n",
        "                                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # shuffling and batching\n",
        "    #dataset_input = dataset_input.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    dataset_input = dataset_input.batch(BATCH_SIZE)\n",
        "    dataset_output = dataset_output.batch(BATCH_SIZE)#.repeat()\n",
        "    \n",
        "    dataset = tf.data.Dataset.zip((dataset_input, dataset_output))\n",
        "    dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VSxrTdOziJS"
      },
      "source": [
        "def get_imageTensor(img, ques):\n",
        "    path = img.decode('utf-8').replace(imageDirectory,'/content/drive/MyDrive/newresults/data'+'/'+'train2014_Numpy').replace('.jpg',\"\") +'.npy'\n",
        "    \n",
        "    img_tensor = np.load(path)\n",
        "    #img_tensor = all_image_dict[img.decode('utf-8')]\n",
        "    \n",
        "    return img_tensor, ques"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgK2tkbEzuTw"
      },
      "source": [
        "BATCH_SIZE = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqW1rlIDyroV"
      },
      "source": [
        "data_set_train = createDataset2(img_data1_tr,ques_data1_tr,anss_data1_tr)\n",
        "data_set_val = createDataset2(img_data1_val,ques_data1_val,anss_data1_val)\n",
        "\n",
        "#print(datrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Vz3odZKS4F"
      },
      "source": [
        "import os\n",
        "currentDirectory = \"/content/drive/MyDrive/newresults/\"\n",
        "os.chdir(currentDirectory)\n",
        "currentDirectory = \"\"\n",
        "dataDirectory = currentDirectory + \"data/\"\n",
        "#imageDirectory = dataDirectory + \"train2014/\"\n",
        "imageDirectory = '/content/train2014/'\n",
        "imageNumpyDirectory = dataDirectory + \"train2014_Numpy\" + ''\n",
        "\n",
        "modelsDirectory = currentDirectory + \"Models/\"\n",
        "\n",
        "\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "\n",
        "#BATCH_SIZE = 64\n",
        "#BATCH_SIZE = 10\n",
        "BUFFER_SIZE = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbnqcvHwKe7U"
      },
      "source": [
        "def callBacksList():\n",
        "    \"\"\"\n",
        "    returns list of callback's\n",
        "    \"\"\"\n",
        "    filepath = modelsDirectory + ModelName + \"/best.hdf5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'auto')\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', patience = 3)\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 4, verbose = 1)\n",
        "\n",
        "    #directory for tensorboard to save evnts\n",
        "    log_dir= modelsDirectory + \"logs/fit/\" + ModelName + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "    print(\"TensorBoard Folder for this Execution\",log_dir)#creating TensorBoard call back,this will write all events to given folder\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
        "\n",
        "    history = tf.keras.callbacks.History()\n",
        "    callbacks_list = [reduce_lr, early_stop, history, tensorboard_callback, checkpoint]\n",
        "    return callbacks_list\n",
        "\n",
        "def Build_BaseModel():\n",
        "    image_input = tf.keras.layers.Input(shape=(7,7,512))\n",
        "    question_input = tf.keras.layers.Input(shape=(opques.shape[1],))\n",
        "\n",
        "    image_conv_layer1 = tf.keras.layers.Conv2D(filters = 4096, kernel_size = 7 , strides = 1, padding = \"valid\", activation = 'relu',\n",
        "                                               kernel_initializer = tf.keras.initializers.he_normal(seed=45))(image_input)\n",
        "\n",
        "    image_flatten = tf.keras.layers.Flatten()(image_conv_layer1)\n",
        "\n",
        "    image_dense_1 = tf.keras.layers.Dense(4096, activation = tf.nn.relu, \n",
        "                                          kernel_initializer = tf.keras.initializers.he_uniform(seed=54))(image_flatten)\n",
        "    \n",
        "    image_dense_2 = tf.keras.layers.Dense(1024, activation = tf.nn.relu, \n",
        "                                          kernel_initializer = tf.keras.initializers.he_uniform(seed=32))(image_dense_1)\n",
        "\n",
        "\n",
        "    # Input 2 Pathway\n",
        "    question_emb = tf.keras.layers.Embedding(input_dim = len(tokenizer.word_index) + 1, output_dim = 300 ,name = \"Embedding_Layer\",\n",
        "                                             embeddings_initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23))(question_input)\n",
        "\n",
        "    question_lstm = tf.keras.layers.LSTM(1024, \n",
        "                                         kernel_initializer = tf.keras.initializers.glorot_uniform(seed=26),\n",
        "                                         recurrent_initializer = tf.keras.initializers.orthogonal(seed=54),\n",
        "                                         bias_initializer=tf.keras.initializers.zeros())(question_emb)\n",
        "\n",
        "    question_flatten = tf.keras.layers.Flatten(name=\"Flatten_lstm\")(question_lstm)\n",
        "\n",
        "    \n",
        "    image_question = tf.keras.layers.Multiply()([image_dense_2, question_flatten])\n",
        "\n",
        "\n",
        "    image_question_dense_1 = tf.keras.layers.Dense(1000, activation = tf.nn.relu,\n",
        "                                                    kernel_initializer = tf.keras.initializers.he_uniform(seed=19))(image_question)\n",
        "    \n",
        "    image_question_dense_2 = tf.keras.layers.Dense(1000, activation = tf.nn.relu, \n",
        "                                                   kernel_initializer = tf.keras.initializers.he_uniform(seed=28))(image_question_dense_1)\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(ans_vocab), activation=tf.nn.softmax, \n",
        "                                   kernel_initializer = tf.keras.initializers.glorot_normal(seed=15))(image_question_dense_2)\n",
        "\n",
        "    # Create Model\n",
        "    model = tf.keras.models.Model(inputs = [image_input, question_input], outputs = output)\n",
        "    # Compile\n",
        "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CREDjwyA1gtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81523025-4ff2-4470-f8c8-548739bc65b5"
      },
      "source": [
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/newresults/my_vqas_model')\n",
        "\n",
        "# Check its architecture\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 7, 7, 512)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 1, 1, 4096)   102764544   input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 21)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 4096)         0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Embedding_Layer (Embedding)     (None, 21, 300)      303300      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 4096)         16781312    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 1024)         5427200     Embedding_Layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         4195328     dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Flatten_lstm (Flatten)          (None, 1024)         0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 1024)         0           dense_1[0][0]                    \n",
            "                                                                 Flatten_lstm[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1000)         1025000     multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1000)         1001000     dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 324)          324324      dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 131,822,008\n",
            "Trainable params: 131,822,008\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmW9eezF1_VH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4edfe3-999c-436a-bf4a-1b32ae6e08b9"
      },
      "source": [
        "!ls /content/drive/MyDrive/newresults/my_vqa_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\tkeras_metadata.pb  saved_model.pb  variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMamuCSz_319"
      },
      "source": [
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBjIVhAkNzLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f9b5b2-7e6f-45d0-b7bf-c70d6bf1b8cb"
      },
      "source": [
        "import datetime\n",
        "new_model.fit(data_set_train, epochs = 60, validation_data = data_set_val)\n",
        "#new_model.fit(datrain, epochs = 20)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 3.9192 - accuracy: 0.1987 - val_loss: 2.8976 - val_accuracy: 0.2348\n",
            "Epoch 2/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 3.9882 - accuracy: 0.1976 - val_loss: 2.9861 - val_accuracy: 0.2348\n",
            "Epoch 3/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 4.2405 - accuracy: 0.1889 - val_loss: 2.7052 - val_accuracy: 0.2348\n",
            "Epoch 4/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 4.0773 - accuracy: 0.1987 - val_loss: 2.8219 - val_accuracy: 0.2348\n",
            "Epoch 5/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 3.9757 - accuracy: 0.1965 - val_loss: 2.7052 - val_accuracy: 0.2348\n",
            "Epoch 6/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 3.8173 - accuracy: 0.2107 - val_loss: 2.2757 - val_accuracy: 0.2348\n",
            "Epoch 7/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 3.4350 - accuracy: 0.2009 - val_loss: 2.1925 - val_accuracy: 0.2348\n",
            "Epoch 8/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 3.2453 - accuracy: 0.2085 - val_loss: 2.1881 - val_accuracy: 0.2348\n",
            "Epoch 9/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 3.1353 - accuracy: 0.2151 - val_loss: 2.2051 - val_accuracy: 0.2304\n",
            "Epoch 10/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 3.1917 - accuracy: 0.1998 - val_loss: 2.1705 - val_accuracy: 0.2304\n",
            "Epoch 11/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 3.0553 - accuracy: 0.2238 - val_loss: 2.1873 - val_accuracy: 0.2304\n",
            "Epoch 12/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.9717 - accuracy: 0.2336 - val_loss: 2.1932 - val_accuracy: 0.2609\n",
            "Epoch 13/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.8242 - accuracy: 0.2500 - val_loss: 2.2151 - val_accuracy: 0.2609\n",
            "Epoch 14/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.8255 - accuracy: 0.2533 - val_loss: 2.2477 - val_accuracy: 0.2609\n",
            "Epoch 15/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.8479 - accuracy: 0.2511 - val_loss: 2.2209 - val_accuracy: 0.2609\n",
            "Epoch 16/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.7787 - accuracy: 0.2587 - val_loss: 2.3247 - val_accuracy: 0.2609\n",
            "Epoch 17/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 3.0245 - accuracy: 0.2489 - val_loss: 2.1409 - val_accuracy: 0.2652\n",
            "Epoch 18/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 3.1339 - accuracy: 0.2555 - val_loss: 2.2528 - val_accuracy: 0.2652\n",
            "Epoch 19/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.7482 - accuracy: 0.2773 - val_loss: 2.2262 - val_accuracy: 0.2652\n",
            "Epoch 20/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.6685 - accuracy: 0.2653 - val_loss: 2.2831 - val_accuracy: 0.2696\n",
            "Epoch 21/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.6563 - accuracy: 0.2675 - val_loss: 2.2686 - val_accuracy: 0.2696\n",
            "Epoch 22/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.7410 - accuracy: 0.2707 - val_loss: 2.0241 - val_accuracy: 0.2696\n",
            "Epoch 23/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.7549 - accuracy: 0.2697 - val_loss: 2.0197 - val_accuracy: 0.2696\n",
            "Epoch 24/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.8331 - accuracy: 0.2751 - val_loss: 1.9126 - val_accuracy: 0.2783\n",
            "Epoch 25/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.6872 - accuracy: 0.2631 - val_loss: 2.4857 - val_accuracy: 0.2783\n",
            "Epoch 26/60\n",
            "458/458 [==============================] - 122s 268ms/step - loss: 2.5766 - accuracy: 0.2773 - val_loss: 1.9540 - val_accuracy: 0.2783\n",
            "Epoch 27/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.7384 - accuracy: 0.2773 - val_loss: 1.8299 - val_accuracy: 0.2609\n",
            "Epoch 28/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.6834 - accuracy: 0.2653 - val_loss: 2.0390 - val_accuracy: 0.2783\n",
            "Epoch 29/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.7274 - accuracy: 0.2751 - val_loss: 1.8214 - val_accuracy: 0.2739\n",
            "Epoch 30/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.6199 - accuracy: 0.2773 - val_loss: 2.1502 - val_accuracy: 0.2783\n",
            "Epoch 31/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.6265 - accuracy: 0.2697 - val_loss: 2.5026 - val_accuracy: 0.2783\n",
            "Epoch 32/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.7485 - accuracy: 0.2620 - val_loss: 1.8981 - val_accuracy: 0.2739\n",
            "Epoch 33/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.9362 - accuracy: 0.2762 - val_loss: 1.7930 - val_accuracy: 0.2565\n",
            "Epoch 34/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.9186 - accuracy: 0.2904 - val_loss: 1.8520 - val_accuracy: 0.2478\n",
            "Epoch 35/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.8783 - accuracy: 0.3013 - val_loss: 1.8342 - val_accuracy: 0.2478\n",
            "Epoch 36/60\n",
            "458/458 [==============================] - 123s 267ms/step - loss: 2.8501 - accuracy: 0.3221 - val_loss: 1.7879 - val_accuracy: 0.2522\n",
            "Epoch 37/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.7728 - accuracy: 0.3275 - val_loss: 1.8433 - val_accuracy: 0.2348\n",
            "Epoch 38/60\n",
            "458/458 [==============================] - 122s 266ms/step - loss: 2.6831 - accuracy: 0.3690 - val_loss: 1.9441 - val_accuracy: 0.2565\n",
            "Epoch 39/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.6516 - accuracy: 0.3876 - val_loss: 2.0910 - val_accuracy: 0.2348\n",
            "Epoch 40/60\n",
            "458/458 [==============================] - 122s 266ms/step - loss: 2.6065 - accuracy: 0.3876 - val_loss: 2.2421 - val_accuracy: 0.2348\n",
            "Epoch 41/60\n",
            "458/458 [==============================] - 122s 266ms/step - loss: 2.5863 - accuracy: 0.3843 - val_loss: 2.0392 - val_accuracy: 0.2478\n",
            "Epoch 42/60\n",
            "458/458 [==============================] - 122s 266ms/step - loss: 2.6089 - accuracy: 0.4017 - val_loss: 2.0887 - val_accuracy: 0.2826\n",
            "Epoch 43/60\n",
            "458/458 [==============================] - 122s 266ms/step - loss: 2.5213 - accuracy: 0.4083 - val_loss: 2.3758 - val_accuracy: 0.2435\n",
            "Epoch 44/60\n",
            "458/458 [==============================] - 122s 266ms/step - loss: 2.5336 - accuracy: 0.4083 - val_loss: 2.1416 - val_accuracy: 0.2696\n",
            "Epoch 45/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.4968 - accuracy: 0.4170 - val_loss: 2.2389 - val_accuracy: 0.2609\n",
            "Epoch 46/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.5408 - accuracy: 0.4148 - val_loss: 2.0798 - val_accuracy: 0.2870\n",
            "Epoch 47/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.5213 - accuracy: 0.4181 - val_loss: 2.0904 - val_accuracy: 0.2826\n",
            "Epoch 48/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.4903 - accuracy: 0.4258 - val_loss: 2.3398 - val_accuracy: 0.2957\n",
            "Epoch 49/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.4166 - accuracy: 0.4225 - val_loss: 2.3663 - val_accuracy: 0.2826\n",
            "Epoch 50/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.5137 - accuracy: 0.4236 - val_loss: 2.2919 - val_accuracy: 0.2783\n",
            "Epoch 51/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.5162 - accuracy: 0.4301 - val_loss: 2.4098 - val_accuracy: 0.2913\n",
            "Epoch 52/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.3372 - accuracy: 0.4312 - val_loss: 2.3543 - val_accuracy: 0.2739\n",
            "Epoch 53/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.3091 - accuracy: 0.4367 - val_loss: 2.3526 - val_accuracy: 0.2739\n",
            "Epoch 54/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.5450 - accuracy: 0.4323 - val_loss: 2.1712 - val_accuracy: 0.2609\n",
            "Epoch 55/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.5272 - accuracy: 0.4312 - val_loss: 2.0533 - val_accuracy: 0.2522\n",
            "Epoch 56/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.5416 - accuracy: 0.4334 - val_loss: 2.1428 - val_accuracy: 0.2696\n",
            "Epoch 57/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.5884 - accuracy: 0.4356 - val_loss: 2.0970 - val_accuracy: 0.2783\n",
            "Epoch 58/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.6276 - accuracy: 0.4312 - val_loss: 2.0800 - val_accuracy: 0.2826\n",
            "Epoch 59/60\n",
            "458/458 [==============================] - 122s 267ms/step - loss: 2.6027 - accuracy: 0.4334 - val_loss: 2.1180 - val_accuracy: 0.2609\n",
            "Epoch 60/60\n",
            "458/458 [==============================] - 123s 268ms/step - loss: 2.5463 - accuracy: 0.4356 - val_loss: 2.1224 - val_accuracy: 0.2739\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc31b858690>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzMl0M7A9XfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1d144b-ed59-4715-f918-7dc5fde28a6a"
      },
      "source": [
        "new_model.save('/content/drive/MyDrive/newresults/Modelskkk')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/newresults/Modelskkk/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/newresults/Modelskkk/assets\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtloHxJ-D8Zd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "0278f1f4-3b15-48ef-a111-9af22770003d"
      },
      "source": [
        "#model.load_weights(modelsDirectory + ModelName + \"/best.hdf5\")\n",
        "test_idx = np.random.randint(len(X_val), size = 3)\n",
        "model_vgg = VGG19_Top()\n",
        "k = 5\n",
        "\n",
        "for idx in test_idx:\n",
        "    test_image_id = X_val['image_id'].values[idx]\n",
        "    test_question = X_val['question'].values[idx]\n",
        "    actual_answer = X_val['multiple_choice_answer'].values[idx]\n",
        "    test_image_path = imageDirectory + 'COCO_train2014_' + '%012d.jpg' % (test_image_id)\n",
        "\n",
        "    test_image_features = model_vgg(tf.expand_dims(load_image(test_image_path)[0], 0))\n",
        "    test_question_features = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([test_question]),padding='post',\n",
        "                                                                           maxlen=question_vector_train.shape[1])\n",
        "    y_pred = model.predict([test_image_features,test_question_features])\n",
        "\n",
        "    class_indices = tf.math.top_k(y_pred,k=k).indices.numpy()\n",
        "    percentages = tf.math.top_k(y_pred,k=k).values.numpy()[0] * 100\n",
        "    predictions = []\n",
        "    for idx,i in enumerate(class_indices[0]):\n",
        "        classes = np.zeros((1,1000))\n",
        "        classes[0][i] = 1\n",
        "        predictions.append((label_encoder.inverse_transform(classes)[0],percentages[idx]))\n",
        "\n",
        "    img=mpimg.imread(test_image_path)\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Question :\", test_question.replace(\"<start> \",\"\").replace(\" <end>\",\"\"))\n",
        "    print(\"Actual Answer: \", actual_answer)\n",
        "    print(\"Top Predicted answers: \",predictions)\n",
        "    print(\"*\"*150)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9240ae0a5b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.load_weights(modelsDirectory + ModelName + \"/best.hdf5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_vgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19_Top\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
          ]
        }
      ]
    }
  ]
}