# -*- coding: utf-8 -*-
"""Visualquestioncheck_sir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x3Jve5WKlcm6n7JGUbzhcCoRlsYbWula
"""

import tensorflow as tf

# We'll generate plots of attention in order to see which parts of an image
# our model focuses on during captioning
#import matplotlib.pyplot as plt

# Scikit-learn includes many helpful utilities
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

import re
import numpy as np
import pandas as pd
import os
import time
import json
import collections
import operator
from glob import glob
from PIL import Image
import pickle
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import numpy as np
from tensorflow import reshape
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Flatten, Dropout, Embedding, LSTM, Activation,ZeroPadding1D,Conv1D

print("Done imports")

annotation_zip = tf.keras.utils.get_file('captions.zip',cache_subdir=os.path.abspath('.'),origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',extract = True)

annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'

name_of_zip = 'train2014.zip'
if not os.path.exists(os.path.abspath('.')+'/' + name_of_zip):
  image_zip = tf.keras.utils.get_file(name_of_zip,cache_subdir=os.path.abspath('.'),origin='http://images.cocodataset.org/zips/train2014.zip',extract=True)
  PATH = os.path.dirname(image_zip)+'/train2014'
else:
  PATH = os.path.abspath('.')+'/train2014'

print(PATH)
#!wget https://s3.amazonaws.com/cvmlp/vqs/mscoco/vqa/v2_Questions_Train_mscoco.zip
#!unzip -a v2_Questions_Train_mscoco.zip
!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip
!unzip -a v2_Annotations_Train_mscoco.zip
!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip
!unzip -a v2_Questions_Train_mscoco.zip

#Storing the captions and image file name in vectors
import collections
import operator

annotation_file = 'v2_mscoco_train2014_annotations.json'

with open(annotation_file,'r') as f:
  annotations = json.load(f)



all_answers = []
all_answers_qids = []
all_img_name_vector = []

for annot in annotations['annotations']:
  #print(annot)
  ans_dic = collections.defaultdict(int)
  for each in annot['answers']:
    diffans = each['answer']
    if diffans in ans_dic:
      #print(each['answer_confidence'])
      if each['answer_confidence']=='yes':
        ans_dic[diffans]+=4
      if each['answer_confidence']=='maybe':
        ans_dic[diffans]+=2
      if each['answer_confidence']=='no':
        ans_dic[diffans]+=1
    else:
      if each['answer_confidence']=='yes':
        ans_dic[diffans]+=4
      if each['answer_confidence']=='maybe':
        ans_dic[diffans]+=2
      if each['answer_confidence']=='no':
        ans_dic[diffans]+=1

  #print(ans_dic)
  most_fav = max(ans_dic.items(),key=operator.itemgetter(1))[0]
  #print(most_fav)
  caption = '<start>' + most_fav + '<end>'  #each['answer']

  image_id = annot['image_id']
  question_id = annot['question_id']
  full_coco_image_path = PATH  + '/COCO_train2014_' + '%012d.jpg' %(image_id)

  all_img_name_vector.append(full_coco_image_path)
  all_answers.append(caption)
  all_answers_qids.append(question_id)

#read the json file
question_file = 'v2_OpenEnded_mscoco_train2014_questions.json'
with open(question_file,'r') as f:
  questions = json.load(f)
all_questions = []
question_ids = []
all_img_name_vector_2 = []

for annot in questions['questions']:
  caption = '<start>' + annot['question'] + '<end>'
  image_id = annot['image_id']
  full_coco_image_path = PATH + '/COCO_train2014_' + '%012d.jpg' %(image_id)
  
  all_img_name_vector_2.append(full_coco_image_path)
  all_questions.append(caption)
  #print(all_questions)
  question_ids.append(annot['question_id'])

print(len(all_img_name_vector),len(all_answers),len(all_answers_qids))
print(all_img_name_vector[10:15],all_answers[10:15],all_answers_qids[10:15])
print(len(all_img_name_vector_2),len(all_questions),len(question_ids))
print(all_img_name_vector_2[10:15],all_questions[10:15],question_ids[10:15])

train_answers,train_questions,img_name_vector = shuffle(all_answers,all_questions,all_img_name_vector,random_state=1)
#train_answers,train_questions,img_name_vector = (all_answers,all_questions,all_img_name_vector)
num_examples=25

train_answers = train_answers[:num_examples]
train_questions = train_questions[:num_examples]
img_name_vector =img_name_vector[:num_examples]

print(img_name_vector[0],train_questions[0],train_answers[0])
print(len(img_name_vector),len(train_questions),len(train_answers))

def load_image(image_path):
  img = tf.io.read_file(image_path)
  img = tf.image.decode_jpeg(img,channels=3)
  img = tf.image.resize(img,(299,299))
  img = tf.keras.applications.inception_v3.preprocess_input(img)
  return img,image_path

image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')
new_input = image_model.input
hidden_layer = image_model.layers[-1].output
image_features_extract_model = tf.keras.Model(new_input,hidden_layer)

# Commented out IPython magic to ensure Python compatibility.
# getting the unique images
# %tb
from tensorflow.python.ops.numpy_ops import np_config
np_config.enable_numpy_behavior()
encode_train = sorted(set(img_name_vector))

        # feel free to change the batch_size according to your system configuration
image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)
image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)

###==================================For reading image in tensor=============================
for img,path in image_dataset:
    #print(path)
    batch_features = image_features_extract_model(img)
    batch_features = tf.reshape(batch_features,(batch_features.shape[0],-1,batch_features.shape[3]))
    
###=====================================For reading image in tensor=============================      
 
    for bf, p in zip(batch_features, path):
         path_of_feature = p.numpy().decode("utf-8")
         
         np.save(path_of_feature, bf.numpy())

# This will find the maximum length of any question in our dataset
def calc_max_length(tensor):
    return max(len(t) for t in tensor)

#choosing the top 10000 words in a dictionary
top_k = 20000
print("Creating question vector")
tokenizer =  tf.keras.preprocessing.text.Tokenizer(num_words=top_k,
                                                          oov_token="<unk>",
                                                          filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
tokenizer.fit_on_texts(train_questions)
train_question_seqs = tokenizer.texts_to_sequences(train_questions)
print("===================================================")
print("=====================================================================")
print("=====================================================================")
print("PRINTING WORD INDEX")
print(tokenizer.word_index)
print("=====================================================================")
print("=====================================================================")
print("PRINTING INDEX WORD")
print(tokenizer.index_word)
ques_vocab = tokenizer.word_index
print(train_question_seqs)
print("=====================================================================")
print("=====================================================================")
print("=====================================================================")
print(ques_vocab)

tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'
# creating the tokenized vectors
train_question_seqs = tokenizer.texts_to_sequences(train_questions)
# padding each vector to the max_length of the captions
        # if the max_length parameter is not provided, pad_sequences calculates that automatically
question_vector = tf.keras.preprocessing.sequence.pad_sequences(train_question_seqs, padding='post')
# calculating the max_length
        # used to store the attention weights
max_length = calc_max_length(train_question_seqs)
print(max_length)
max_q = max_length

from numpy import array
from numpy import argmax

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

data = train_answers
values = array(data)
print(values[:10])

label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(values)

ans_vocab = {1: i for i,l in enumerate(label_encoder.classes_)}
print("PRINTING ANSWER VOCABULARY")
print("==========================================================================================")
print("==========================================================================================")
print("==========================================================================================")
print(ans_vocab)

onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded),1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print("PRINTING ONE HOT ENCODED ANSWERS")
print("=============================================================================================")
print("==========================================================================================")
print("==========================================================================================")

print(onehot_encoded[0],len(onehot_encoded))

answer_vector = onehot_encoded
len_ans_vocab = len(onehot_encoded[0])

print(answer_vector)
print(len(question_vector[0]),len(answer_vector[0]))
class_label = onehot_encoder.inverse_transform(onehot_encoded)
print("*******************************************printing labels*******************************************************")
print(class_label)

img_name_train,img_name_val,question_train,question_val,answer_train,answer_val = train_test_split(img_name_vector,question_vector,answer_vector,test_size=0.2,random_state=0)
print(len(img_name_train),len(img_name_val),len(question_train),len(question_val),len(answer_train),len(answer_val))
question_train.shape

BATCH_SIZE = 16 #64
BUFFER_SIZE = 1 #1000
#embedding_dim = 256
#units = 512
#vocab_size =len(tokenizer.word_index)+1
num_steps = len(img_name_train) // BATCH_SIZE
#shape of the vector extracted from InceptionV3 is (64,2048)
#these two variables represent that
features_shape = 2048
attention_features_shape = 64

!pip install fontstyle

import fontstyle

import cv2
from google.colab.patches import cv2_imshow

def map_func(img_name,cap,ans):
  #print(os.getcwd())
      #print(img_name)
      output = str(img_name,'UTF-8')
      #print(output)
      view_img = cv2.imread(output)
      cv2_imshow(view_img)
      print("                                                                       ")
      print("                                                                       ")
      img_tensor = np.load(img_name.decode('utf-8')+'.npy')
      #Question
      cap_emp = []
      cap_emp.append(cap)
      from tensorflow.keras.preprocessing.text import Tokenizer
      tok = Tokenizer(num_words=2000)
      tok.fit_on_sequences(cap_emp)
      question_text = tokenizer.sequences_to_texts(cap_emp)
      #print(question_text)
      bold_question = fontstyle.apply(question_text,'bold/Italic/green')
      #a_number = 1
      question_as_string = str(bold_question)
      right_aligned_question = question_as_string.rjust(20)
      que = "QUESTION:"
      que_des = fontstyle.apply(que,'bold/black')
      print(que_des)
      #print("                                                                       ")
      #print("                                                                       ")
      print(right_aligned_question)

      #Answer
      ans_sen = label_encoder.inverse_transform([argmax(ans)])
      bold_answer = fontstyle.apply(ans_sen,'bold/Italic/purple')
      anse = "ANSWER:"
      anse_des = fontstyle.apply(anse,'bold/black')
      print(anse_des)
      print(bold_answer)
      #print(cap)
      #print(ans)
      #print("endere here")
      return img_name,img_tensor, cap,ans

def map_print(self,t1, t2, t3, t4):
        print("yahan hoo main")
        print(t1,t2,t3,t4)
        return t1,t2,t3,t4

dataset = tf.data.Dataset.from_tensor_slices((img_name_train, question_train.astype(np.float32), answer_train.astype(np.float32)))
#filenames = tf.data.Dataset
#dataset = dataset.interleave(tf.data.TFRecordDataset(dataset))
#dataset = tf.data.Dataset.from_tensor_slices((img_name_train, question_train.astype(np.float32), answer_train.astype(np.float32)))
print(dataset)
        # using map to load the numpy files in parallel
dataset = dataset.map(lambda item1, item2, item3: tf.numpy_function(map_func, [item1, item2, item3], [tf.string, tf.float32, tf.float32, tf.float32]),
                  num_parallel_calls=tf.data.experimental.AUTOTUNE)
#print(dataset)
#dataset = dataset.map(lambda item1, item2, item3, item4: tf.numpy_function(map_print, [item1, item2, item3, item4], [tf.string, tf.float32, tf.float32, tf.float32]),
                  #num_parallel_calls=tf.data.experimental.AUTOTUNE)
        # shuffling and batching
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)        
#dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
#dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 ####

test_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, question_val.astype(np.float32), answer_val.astype(np.float32)))

        # using map to load the numpy files in parallel
test_dataset = test_dataset.map(lambda item1, item2, item3: tf.numpy_function(
                  map_func, [item1, item2, item3], [tf.string, tf.float32, tf.float32, tf.float32]),
                  num_parallel_calls=tf.data.experimental.AUTOTUNE)

        # shuffling and batching
#test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)
#test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

class PrependModel(tf.keras.Model):
	def __init__(self, ans_len, que_len):
		super(PrependModel, self).__init__()
		self.flatten = tf.keras.layers.Flatten()
		self.condense = tf.keras.layers.Dense(64, activation="relu")
		self.embedding = tf.keras.layers.Embedding(input_dim=que_len + 1,output_dim=64)
		self.gru = tf.keras.layers.GRU(256, return_sequences=False, return_state=False)
		self.logits = tf.keras.layers.Dense(ans_len, activation="softmax")

	def call(self, x, sents):
		flat_out = self.flatten(x)
		cond_out = self.condense(flat_out)
		cond_out = tf.expand_dims(cond_out, axis=1)
		sents = self.embedding(sents)
		input_s = tf.concat([cond_out, sents], axis=1)
		output = self.gru(input_s)
		final = self.logits(output)
		return final

	def init_state(self, batch_s):
		return tf.zeros((batch_s, 256))

class AppendImageAsWordModel(tf.keras.Model):
  def __init__(self,embedding_size,rnn_size,output_size):
    super(AppendImageAsWordModel,self).__init__()
    #self.embedding_size = 128
    #self.rnn_size = 512
    self.flatten = tf.keras.layers.Flatten()
    self.condense = tf.keras.layers.Dense(embedding_size,activation='relu')

    #add embedding layers for questions
    self.embedding = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1,output_dim=embedding_size)

    #create the input
    self.gru = tf.keras.layers.GRU(rnn_size,return_sequences=False,return_state=False)
    self.logits = tf.keras.layers.Dense(output_size,activation='softmax')


  #new call without indent
  def call(self,x,sents,hidden):
    flattended_output = self.flatten(x)
    condensed_out = self.condense(flattended_output)
    #print("condensed_output shape",condensed_out.shape)
    condensed_out = tf.expand_dims(condensed_out,axis=1)
    #print("condensed_output shape after expand dimension",condensed_out.shape)
    sents = self.embedding(sents)
    #print("sents shape",sents.shape)
    input_s = tf.concat([sents,condensed_out],axis=1)
    #print("input shape",input_s.shape)
    output = self.gru(input_s,initial_state=hidden)
    final_output = self.logits(output)
    #print("final output shape",final_output.shape)
    return final_output
    
  def init_state(self,batch_size,rnn_size):
    return tf.zeros((batch_size,rnn_size))

!pip install fontstyle

import fontstyle

# Commented out IPython magic to ensure Python compatibility.
# %tb
import numpy
import matplotlib.pyplot as plt

train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
test_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

append_image_word_model = AppendImageAsWordModel(256,256,len_ans_vocab)
crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
def calc_loss(labels,logits):
  return crossentropy(labels,logits)

#optimizer_append = tf.compat.v1.train.AdamOptimizer
optimizer_append = tf.keras.optimizers.Adam() 
tf.compat.v1.enable_eager_execution()
#@tf.function
def train_step(inp_imgs,input_sents,labels,initial_state):
    with tf.GradientTape() as tape:
        result=[]
        my_model_output = append_image_word_model(inp_imgs,input_sents,initial_state)
        loss = calc_loss(labels,my_model_output)
        predictions = append_image_word_model(inp_imgs,input_sents,initial_state)
        #print("Prediction",predictions)
        #num_pre = predictions.numpy()
        max_pred = tf.math.argmax(predictions,axis =0)
        #print("TYPE OF MAX_PRED",type(max_pred))
        #print("============MAXIMUM VALUE OF PREDICTION",max_pred)
        max_pred_onehot = tf.one_hot(max_pred,depth=predictions.shape[1])
        #print("max_pred one hot array",max_pred_onehot)
        
        #print("============MAXIMUM VALUE OF PREDICTION at index 3",max_pred[3])
        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()
        #print("predicted id using categorical",predicted_id)
        result.append(tokenizer.index_word[predicted_id])
        #print("Word of predicted id",result)
        #train_loss = loss_function(y_true=labels, y_pred=logits)
    variables = append_image_word_model.trainable_variables
    gradients = tape.gradient(loss,variables)
    optimizer_append.apply_gradients(zip(gradients,variables))  
    
    return loss
EPOCHS = 40
for epoch in range(EPOCHS):
  init_states = append_image_word_model.init_state(BATCH_SIZE,256)
  #print(init_states)
  #for batch,(img_ques_ans) in enumerate(dataset):
  for (batch,(img_name,img_tensor,question,answer)) in enumerate(dataset):
    loss = train_step(img_tensor,question,answer,init_states)
  if epoch%10==0:
    
  
# format text
    text = fontstyle.apply('PRINTING EPOCHS AND LOSS', 'bold/Italic/red/GREEN_BG')
  
# display text
    epoloss_as_string = str(text)
    right_aligned_epoloss = epoloss_as_string.rjust(180)

    print(right_aligned_epoloss)
    print(text)
    print("Epoch #%d,Loss %.4f" %(epoch,loss))

pip install scipy==1.1.0

import scipy

# Commented out IPython magic to ensure Python compatibility.
# %tb
import numpy
from PIL import Image
from numpy import asarray
import io
from keras.preprocessing.image import array_to_img
from scipy.misc.pilutil import toimage
from matplotlib import cm
from google.colab.patches import cv2_imshow
import cv2

train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
test_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

append_image_word_model = AppendImageAsWordModel(256,256,len_ans_vocab)
crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
def calc_loss(labels,logits):
  return crossentropy(labels,logits)

#optimizer_append = tf.compat.v1.train.AdamOptimizer
optimizer_append = tf.keras.optimizers.Adam() 
tf.compat.v1.enable_eager_execution()
#@tf.function
def test_step(inp_imgs,input_sents,labels,initial_state):
    with tf.GradientTape() as tape:
        result=[]
        question=[]
        my_model_output = append_image_word_model(inp_imgs,input_sents,initial_state)
        loss = calc_loss(labels,my_model_output)
        predictions = append_image_word_model(inp_imgs,input_sents,initial_state)

        #for inp_imgs in enumerate(test_dataset):
          #for single in inp_imgs:
            #print(single)
        
        
        
        for (batch,(each_img,each_sent,each_ans,each_int)) in enumerate(test_dataset):
            #print(each_img)
            for each_img_one in each_img:
              #print(each_img_one)
      
              each_img_one = each_img_one.numpy().decode("utf-8") # .numpy() retrieves data from eager tensor
              each_str_one = str(each_img_one)
              #print(each_str_one)
              each_str_one_read = cv2.imread(each_str_one)
              cv2_imshow(each_str_one_read)
              max_pred = tf.math.argmax(predictions,axis =0)
              #print("TYPE OF MAX_PRED",type(max_pred))
              #print("============MAXIMUM VALUE OF PREDICTION",max_pred)
            
              max_pred = tf.math.argmax(predictions,axis =0)
              #print("TYPE OF MAX_PRED",type(max_pred))
              #print("============MAXIMUM VALUE OF PREDICTION",max_pred)
              max_pred_onehot = tf.one_hot(max_pred,depth=predictions.shape[1])
              #print("max_pred one hot array",max_pred_onehot)
        
              #print("============MAXIMUM VALUE OF PREDICTION at index 3",max_pred[3])
              predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()
              #print("predicted id using categorical",predicted_id)
              result.clear()
              result.append(tokenizer.index_word[predicted_id])
              
              print("Word of predicted id",result)
              #train_loss = loss_function(y_true=labels, y_pred=logits)

####For Question
              rid = np.random.randint(0, len(input_sents))
              image = inp_imgs[rid]
              question = ' '.join([tokenizer.index_word[i]
                        for i in question_val[rid] if i not in [0]])
              print('Question of image:', question)
              #print('Prediction Caption:', ' , '.join(question))

   ###For answer
              rid = np.random.randint(0, len(inp_imgs))
              image = inp_imgs[rid]
              real_answer = ' '.join([tokenizer.index_word[i]
                        for i in answer_val[rid] if i not in [0]])
        #Image.open(inp_imgs)
        #result, attention_plot = evaluate(image)

              print('Real Answer:', real_answer)
              #print('Prediction Answer:', ' , '.join(result))

###       FINAL IMAGE ,ANSWER AND WORD JOIN  ######################      
        
    variables = append_image_word_model.trainable_variables
    gradients = tape.gradient(loss,variables)
    optimizer_append.apply_gradients(zip(gradients,variables))  
    
    return loss


EPOCHS = 40
for epoch in range(EPOCHS):
  init_states = append_image_word_model.init_state(BATCH_SIZE,256)
  #print(init_states)
  #for batch,(img_ques_ans) in enumerate(dataset):
  for (batch,(img_name,img_tensor,question,answer)) in enumerate(test_dataset):
    #loss = train_step(img_tensor,question,answer,init_states)
    #print(img_tensor)
    #(question)
    loss = test_step(img_tensor,question,answer,init_states)
    #accuracy = train_accuracy_metric(answer,init_states)  
    #print("Accuracy",accuracy)
  if epoch%10==0:
    print("Epoch #%d,Loss %.4f" %(epoch,loss))